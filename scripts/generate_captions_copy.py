import os
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
import json

SUPPORTED_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.webp', '.bmp', '.tiff')

def load_model(model_path):
    print("æ­£åœ¨åŠ è½½æ¨¡å‹å’ŒTokenizer...")
    try:
        model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
        ).eval().cuda()

        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )

        print("âœ… æ¨¡å‹å’ŒTokenizeråŠ è½½å®Œæˆã€‚")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æˆ–Tokenizeræ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise


def generate_caption(model, tokenizer, image_path, question):
    try:
        image = Image.open(image_path).convert('RGB')
        msgs = [{'role': 'user', 'content': [image, question]}]
        res = model.chat(image=None, msgs=msgs, tokenizer=tokenizer)
        return str(res).strip()
    except Exception as e:
        print(f"âŒ å›¾ç‰‡ {image_path} å¤„ç†å¤±è´¥: {e}")
        return None


def process_all_images(model, tokenizer, image_dir, caption_output_dir, question):
    os.makedirs(caption_output_dir, exist_ok=True)

    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(SUPPORTED_EXTENSIONS)]
    total_images = len(image_files)
    print(f"ğŸ“· å…±æ‰¾åˆ° {total_images} å¼ å›¾ç‰‡å¾…å¤„ç†ã€‚")

    all_captions_metadata = []

    for i, image_filename in enumerate(image_files):
        print(f"ğŸ–¼ï¸ å¤„ç†ç¬¬ {i+1}/{total_images} å¼ : {image_filename}")
        image_path = os.path.join(image_dir, image_filename)
        base_filename, _ = os.path.splitext(image_filename)
        caption_txt_path = os.path.join(caption_output_dir, f"{base_filename}.txt")

        # è·³è¿‡å·²å­˜åœ¨çš„captionæ–‡ä»¶
        # if os.path.exists(caption_txt_path):
        #     print(f"âš ï¸ Caption å·²å­˜åœ¨: {caption_txt_path}ï¼Œè·³è¿‡ã€‚")
        #     continue

        caption = generate_caption(model, tokenizer, image_path, question)

        if not caption:
            caption = "ghibli style"
            print(f"âš ï¸ ç©ºcaptionä½¿ç”¨é»˜è®¤æ–‡æœ¬: {caption}")

        # ä¿å­˜caption
        try:
            with open(caption_txt_path, 'w', encoding='utf-8') as f:
                f.write(caption)
            print(f"âœ… Caption ä¿å­˜åˆ°: {caption_txt_path}")
        except Exception as e:
            print(f"âŒ å†™å…¥captionæ–‡ä»¶å¤±è´¥: {e}")

        all_captions_metadata.append({
            "image_filename": image_filename,
            "caption_file": os.path.basename(caption_txt_path),
            "caption": caption
        })

    return all_captions_metadata


def save_metadata(metadata, output_dir):
    if not metadata:
        return
    metadata_path = os.path.join(output_dir, "_metadata_captions.jsonl")
    try:
        with open(metadata_path, 'w', encoding='utf-8') as f:
            for item in metadata:
                f.write(json.dumps(item) + '\n')
        print(f"ğŸ“ Captionå…ƒæ•°æ®ä¿å­˜è‡³: {metadata_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")


if __name__ == "__main__":
    model_path = "/root/autodl-tmp/models/MiniCPM-V-2_6"
    image_dir = "/root/autodl-tmp/HuggingFace_Datasets/bdd100k/fulldata_test/images"
    caption_output_dir = os.path.join(os.path.dirname(image_dir), "captions")
    question = "This is a picture of a real road scene. Please give descriptive synthetic captions for the image for LoRA fine-tuning." \
    " Please only output the prompt, do not output any thought process."

    try:
        model, tokenizer = load_model(model_path)
        metadata = process_all_images(model, tokenizer, image_dir, caption_output_dir, question)
        save_metadata(metadata, caption_output_dir)
        print("ğŸ‰ æ‰€æœ‰å›¾ç‰‡å¤„ç†å®Œæˆï¼")
    except Exception as main_error:
        print(f"ğŸš¨ ä¸»æµç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {main_error}")
